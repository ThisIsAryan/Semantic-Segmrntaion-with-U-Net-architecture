{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fO9jJMQWLXe"
   },
   "source": [
    "# Semantic Segmentation on Helen Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJg4jo6lWPte"
   },
   "source": [
    "# Steps followed\n",
    "\n",
    "1. Importing necessary libraries\n",
    "2. Looking at directory structure and making desired shiftings\n",
    "3. Data Preprocessing\n",
    "4. Defining Train and Test Dataloaders\n",
    "5. Defining model\n",
    "6. Defining Dice Loss and optimizer\n",
    "7. Performing Forward Propagation\n",
    "8. Visualizing predictions\n",
    "9. Predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge google-colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riUIdSCDVHfk",
    "outputId": "c1005278-b54a-413c-c352-3b136aa8ba75"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'termios'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b80391d2b2cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\colab\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_installation_commands\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_shell_customizations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_tensorflow_magics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\colab\\_system_commands.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlocale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mselect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msignal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\pty.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"openpty\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"fork\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"spawn\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\tty.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Author: Steen Lumholt.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtermios\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"setraw\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"setcbreak\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'termios'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "892-8C0zWVZ2"
   },
   "source": [
    "## Step 1. Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_31500FQVbpv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dXmkD3ZC4dLS",
    "outputId": "510bf110-a3bf-4a33-da4d-52d0e9863c04"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/drive/MyDrive/Datasets')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdRig9CyWY1t"
   },
   "source": [
    "## Step 2. Looking at directory structure\n",
    "\n",
    "### a) Image files directory tree\n",
    "<pre>.\n",
    "└── helenstar_release\n",
    "     ├── train\n",
    "     │   ├── image.jpg\n",
    "     │   ├── label.png\n",
    "     ├   ├── viz.jpg\n",
    "     │   └── ... (1999 sets of 3 images i.e 5997 images total)           \n",
    "     └── test\n",
    "         ├── image.jpg\n",
    "         ├── label.png`\n",
    "         ├── viz.jpg\n",
    "         └── ... (100 sets of 3 images i.e 300 images total)</pre>\n",
    "\n",
    "\n",
    "### b) Perform necessary shiftings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GMvtOQkWZxz"
   },
   "outputs": [],
   "source": [
    "train_images = []\n",
    "train_labels = []\n",
    "for file in tqdm(os.listdir(os.path.join(os.getcwd() ,'helenstar_release/train'))):\n",
    "    \n",
    "    if \"image\" in file:\n",
    "        train_images.append(os.path.join(os.getcwd() ,'helenstar_release/train', file))\n",
    "    elif \"label\" in file:\n",
    "        train_labels.append(os.path.join(os.getcwd() ,'helenstar_release/train', file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wMGJTRuWkhU"
   },
   "outputs": [],
   "source": [
    "test_images = []\n",
    "test_labels = []\n",
    "for file in tqdm(os.listdir(os.path.join(os.getcwd() ,'helenstar_release/test'))):\n",
    "    \n",
    "    if \"image\" in file:\n",
    "        test_images.append(os.path.join(os.getcwd() ,'helenstar_release/test', file))\n",
    "    elif \"label\" in file:\n",
    "        test_labels.append(os.path.join(os.getcwd() ,'helenstar_release/test', file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRA-qXULWquU"
   },
   "outputs": [],
   "source": [
    "for train_image in train_images:\n",
    "    shutil.copy(train_image , os.path.join(os.getcwd() , 'splitted_data/train/images/'))\n",
    "    \n",
    "for train_label in train_labels:\n",
    "    shutil.copy(train_label , os.path.join(os.getcwd() , 'splitted_data/train/labels/'))\n",
    "    \n",
    "for test_image in test_images:\n",
    "    shutil.copy(test_image , os.path.join(os.getcwd() , 'splitted_data/test/images/'))\n",
    "    \n",
    "for test_label in test_labels:\n",
    "    shutil.copy(test_label , os.path.join(os.getcwd() , 'splitted_data/test/labels/'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdkXQGAiW_Au"
   },
   "source": [
    "### c) New directory structure\n",
    "\n",
    "<pre>.\n",
    "└── splitted_Data\n",
    "        ├── train\n",
    "        │   ├── images\n",
    "        │   │   ├── image1.jpg\n",
    "        │   │   ├── image2.jpg\n",
    "        │   │   └── ... (1999 files)\n",
    "        │   └── labels\n",
    "        │       ├── label1.jpg\n",
    "        │       ├── label2.jpg\n",
    "        │       └── ... (1999 files)       \n",
    "        │           \n",
    "        └── test\n",
    "            ├── images\n",
    "            │   ├── image1.jpg\n",
    "            │   ├── image2.jpg\n",
    "            │   └── ... (100 files)\n",
    "            └── labels\n",
    "                ├── label1.jpg\n",
    "                ├── label2.jpg\n",
    "                └── ... (100 files)</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWJIMzozXeXe"
   },
   "source": [
    "# Step 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8ugi-ivW8RQ"
   },
   "outputs": [],
   "source": [
    "def one_hot(arr):\n",
    "    \n",
    "    arr = torch.from_numpy(arr)\n",
    "    one_hot=F.one_hot(arr.to(torch.int64),num_classes=11)\n",
    "    return np.transpose(np.array(one_hot),(2,0,1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53xt5KcqXhTm"
   },
   "outputs": [],
   "source": [
    "class HelenDataset(Dataset):\n",
    "    def __init__(self, label_dir, img_dir):\n",
    "        self.img_list=os.listdir(img_dir)\n",
    "        self.label_list=os.listdir(label_dir)\n",
    "        self.label_dir = label_dir\n",
    "        self.img_dir = img_dir\n",
    "        self.label_list.sort()\n",
    "        self.img_list.sort()\n",
    "        #print(self.label_list)\n",
    "        #print(self.img_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, self.img_list[idx])\n",
    "        image = Image.open(img_path)\n",
    "        image = image.resize((256,256), resample=PIL.Image.BICUBIC)\n",
    "\n",
    "        image = np.array(image)/255\n",
    "        image = np.transpose(image,(2,0,1))\n",
    "        image = torch.tensor(image, dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "        label_path = os.path.join(self.label_dir, self.label_list[idx])\n",
    "        label = Image.open(label_path)\n",
    "        label = label.resize((256,256), resample=PIL.Image.NEAREST)\n",
    "\n",
    "        label = np.array(label)\n",
    "        label = one_hot(label)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "        sample = [image, label]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5XAqDKlhbkC"
   },
   "source": [
    "## Step 4. Defining Train and Test Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bdm8fCOxXo82"
   },
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "\n",
    "\n",
    "train_label_path= os.path.join(curr_dir,\"splitted_data\", \"data/train/labels\")\n",
    "train_image_path= os.path.join(curr_dir,\"splitted_data\", \"data/train/images\")\n",
    "\n",
    "training_data = HelenDataset(label_dir=train_label_path, img_dir=train_image_path)\n",
    "\n",
    "test_label_path= os.path.join(curr_dir, \"splitted_data\", \"data/test/labels\")\n",
    "test_image_path= os.path.join(curr_dir, \"splitted_data\", \"data/test/images\")\n",
    "\n",
    "test_data = HelenDataset(label_dir=test_label_path, img_dir=test_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmcyYpOwXzxm"
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_dataloader = DataLoader(training_data, batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfwBS7NhX04T"
   },
   "outputs": [],
   "source": [
    "for images,labels in train_dataloader:\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_hBOVw-CYxvR",
    "outputId": "bf9e2bb4-552e-4c10-ab0f-76b273ecefdf"
   },
   "outputs": [],
   "source": [
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "lkv4nUJrY3J_",
    "outputId": "31ab8944-82a0-45c3-d486-2b40166b43ef"
   },
   "outputs": [],
   "source": [
    "im = make_grid(images , nrow = 5)\n",
    "plt.imshow(np.transpose(im , (1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOels2MrhjYo"
   },
   "source": [
    "# Step 5. Defining model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0HTPp7JY7WA"
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding = 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(negative_slope=0.05, inplace=True),\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding = 1 ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(negative_slope=0.05, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igkvWLj6Y-Iw"
   },
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLCLNEDFY-zH"
   },
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    \n",
    "    def __init__(self,in_channels,out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "        \n",
    "    def forward(self, from_up , from_skip):\n",
    "        \n",
    "        from_up = self.up(from_up)\n",
    "        size_up = from_up.size()[2]\n",
    "        size_skip = from_skip.size()[2]\n",
    "        \n",
    "        diff = size_skip - size_up\n",
    "        \n",
    "        from_up = F.pad( from_up , [diff//2 , diff - diff//2 ,\n",
    "                                   diff//2 , diff - diff//2])\n",
    "        \n",
    "        x = torch.cat([from_skip , from_up ] , dim = 1)\n",
    "        \n",
    "        return self.conv(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuT7zfoRZACF"
   },
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KGfDXvkZBQc"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_channels = 3 , n_classes = 11):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        \n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        \n",
    "        \n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256 )\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        \n",
    "        x = nn.Softmax(dim = 1)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EebsazloZCc1"
   },
   "outputs": [],
   "source": [
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fvaO-tRUZEDP",
    "outputId": "0f5006a1-f29a-45b2-996c-50de86c73866"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device=torch.device('cuda')\n",
    "else:\n",
    "  device=torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swe4gV8XZFNu",
    "outputId": "f9fb3cce-edb0-4af8-de92-c862f3ec2593"
   },
   "outputs": [],
   "source": [
    "#!pip install torchsummary\n",
    "from torchsummary import summary\n",
    "summary(model, (3, 256, 256) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZuOupAjfZNEa",
    "outputId": "8e44bdd9-10d5-4e50-9116-437a88966c0a"
   },
   "outputs": [],
   "source": [
    "pred = model.forward(images.cuda())\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IqqmhelrZQWX",
    "outputId": "ca599f2d-cb90-4ae7-9809-09a1fa536603"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"MyModel5.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sMwbjkjhtUQ"
   },
   "source": [
    "# Step 6. Dice Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZH_Le8yWhsSw"
   },
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self , channels = 11 , eps = 1e-7 ):\n",
    "        super(DiceLoss,self).__init__()        \n",
    "        self.channels = channels\n",
    "        self.eps = eps\n",
    "        \n",
    "        \n",
    "    def forward(self , predicted , target):\n",
    "        \n",
    "        mul = predicted * target\n",
    "        num = torch.sum(mul , dim=(2,3))\n",
    "        den = 0.7*torch.sum(predicted , dim = (2,3)) + 0.3*torch.sum(target, dim=(2,3)) + self.eps\n",
    "        loss = torch.sum( 1 - torch.div(num,den))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMDXlyvLZbYh"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9lEzU3PiBGB"
   },
   "source": [
    "# Step 7. Performing Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xP1SIfrSZko3"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "running_loss = 10*11\n",
    "training_losses = []\n",
    "for epochs in tqdm(range(epochs)):\n",
    "  for b , (images,labels) in enumerate(train_dataloader):\n",
    "    \n",
    "    images=images.to(device)\n",
    "    labels=labels.to(device)\n",
    "    preds = model(images)               \n",
    "    loss = DiceLoss().forward(preds, labels)\n",
    "\n",
    "    optimizer.zero_grad() #set gradients 0\n",
    "    loss.backward() #calculate gradients\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.01, norm_type=2.0)\n",
    "    optimizer.step() #update weights\n",
    "\n",
    "    if b % 20 == 1:\n",
    "      print(\"mini_batch_ \", b , \"   loss \",loss)\n",
    "      training_losses.append(loss)\n",
    "\n",
    "    del images\n",
    "    del labels\n",
    "\n",
    "  torch.save(model.state_dict(), '/content/drive/MyDrive/Datasets/MyModel5.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbgZXdVHiNAg"
   },
   "source": [
    "# Step 8. Visualizing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIJgHvpZZzo8"
   },
   "outputs": [],
   "source": [
    "preds = model(images.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "B3ktZQSXfqXZ",
    "outputId": "49f88c3c-2bab-4935-bc7a-f2912d573f02"
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(images[8] , (1,2,0)))\n",
    "\n",
    "#plt.savefig('image3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "KeQkLyabZqPW",
    "outputId": "434a07bc-a42a-4b4e-ce52-c5b4f73b533d"
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.argmax(labels , dim=1)[8])\n",
    "#plt.savefig('label3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "qD7HG3lwbR36",
    "outputId": "cdc9c70c-fb67-4e33-f2e2-75a97a447015"
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.argmax(preds.cpu() , dim=1)[8])\n",
    "#plt.savefig('pred3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsIfJsI6iS2x"
   },
   "source": [
    "# Step 9. Predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-G33QPydmoy"
   },
   "outputs": [],
   "source": [
    "test_imgpath=os.path.join(curr_dir,\"splitted_data\",'test/images')\n",
    "test_labelpath=os.path.join(curr_dir,\"splitted_data\",'test/labels')\n",
    "test_predpath=os.path.join(curr_dir,\"splitted_data\",'test/preds')\n",
    "\n",
    "test_imglist=os.listdir(os.path.join(curr_dir,\"splitted_data\",'test/images'))\n",
    "test_labellist=os.listdir(os.path.join(curr_dir,\"splitted_data\",'test/labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYpokS_Vd3Of"
   },
   "outputs": [],
   "source": [
    "test_imglist.sort()\n",
    "test_labellist.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZiP46Jud7v_"
   },
   "outputs": [],
   "source": [
    "for i in range(len(test_labellist)):\n",
    "  img=Image.open(os.path.join(test_imgpath, test_imglist[i]))\n",
    "  img=np.array(img)\n",
    "  img=np.transpose(img,(2,0,1))\n",
    "  img_tensor=torch.tensor(img,dtype=torch.float32)\n",
    "  img_tensor=img_tensor.unsqueeze(0)\n",
    "\n",
    "  label=Image.open(os.path.join(test_labelpath, test_labellist[i]))\n",
    "  label=np.array(label)\n",
    "  label_tensor=torch.tensor(label,dtype=torch.float32)\n",
    "  label_tensor=label_tensor.unsqueeze(0)\n",
    "\n",
    "  pred=model(img_tensor.to(device))\n",
    "\n",
    "  pred=np.array(pred.clone().detach().cpu())\n",
    "  pred=np.argmax(pred[0] , axis=0)\n",
    "  pred=pred.astype(np.uint8)\n",
    "\n",
    "  pred=Image.fromarray(pred)\n",
    "  pred=pred.save(os.path.join(test_predpath, test_labellist[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "anLjVfSIazNP",
    "outputId": "06d89b4d-6619-45c1-a774-6bddc0c02d2a"
   },
   "outputs": [],
   "source": [
    "!python3 /content/drive/MyDrive/Datasets/splitted_data/f1_score.py \"/content/drive/MyDrive/Datasets/splitted_data/test/labels\" \"/content/drive/MyDrive/Datasets/splitted_data/test/preds\" \"/content/drive/MyDrive/Datasets/splitted_data/label_names.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "If1Ykq8846r6"
   },
   "outputs": [],
   "source": [
    "for test_images,test_labels in test_dataloader:\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaKX2kYp5DCd"
   },
   "outputs": [],
   "source": [
    "test_pred = model(test_images.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "ggHDUQlG5LWd",
    "outputId": "2882e322-0c74-4092-b9c5-f44e53c0671d"
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(test_images[9] , (1,2,0)))\n",
    "\n",
    "plt.savefig('test_image3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "6P8vdSee5SQC",
    "outputId": "efca1982-7f30-461a-9634-95de64ee287e"
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.argmax(test_labels , dim=1)[9])\n",
    "plt.savefig('test_label3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "epvmFo4e5Uhx",
    "outputId": "53b0f2a1-4136-4bf6-fb16-bc574043080c"
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.argmax(test_pred.cpu() , dim=1)[9])\n",
    "plt.savefig('test_pred3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tdzf5SCB5o0J"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPWv301PVmdovteEUA8E5RP",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "UNet_Final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
